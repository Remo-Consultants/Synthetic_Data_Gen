{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# COT Synthetic Data Generator - Kaggle\n",
                "\n",
                "This notebook runs the COT Synthetic Dataset Generator on Kaggle.\n",
                "\n",
                "**Setup Steps:**\n",
                "1. Settings â†’ Accelerator â†’ GPU T4 x2\n",
                "2. Settings â†’ Internet â†’ ON\n",
                "3. Settings â†’ Persistence â†’ ON\n",
                "4. Add your dataset (if uploaded) or use GitHub\n",
                "5. Run all cells in order\n",
                "\n",
                "**Advantages:**\n",
                "- 2x T4 GPUs (16GB total)\n",
                "- 9-hour sessions\n",
                "- Auto-saved outputs\n",
                "- Better for large batches\n",
                "\n",
                "**Estimated Time:** 1-2 hours for 50 seeds"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Project Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import shutil\n",
                "\n",
                "# ============================================\n",
                "# OPTION A: From Kaggle Dataset\n",
                "# ============================================\n",
                "# 1. Upload your project as a Kaggle dataset first\n",
                "# 2. Add it to this notebook (Add Data â†’ Your Datasets)\n",
                "# 3. Update the path below\n",
                "\n",
                "dataset_path = '/kaggle/input/cot-synthetic-data-generator/'\n",
                "\n",
                "# Create working directory\n",
                "!mkdir -p /kaggle/working/project\n",
                "%cd /kaggle/working/project\n",
                "\n",
                "# Extract if tar.gz\n",
                "if os.path.exists(f'{dataset_path}synthetic-data-gen.tar.gz'):\n",
                "    !tar -xzf {dataset_path}synthetic-data-gen.tar.gz -C /kaggle/working/project\n",
                "    print(\"âœ“ Extracted from tar.gz\")\n",
                "elif os.path.exists(dataset_path):\n",
                "    # Copy all files from dataset\n",
                "    !cp -r {dataset_path}* /kaggle/working/project/\n",
                "    print(\"âœ“ Copied from dataset\")\n",
                "\n",
                "# ============================================\n",
                "# OPTION B: From GitHub\n",
                "# ============================================\n",
                "# Uncomment and update with your repo URL\n",
                "# REPO_URL = \"https://github.com/YOUR_USERNAME/YOUR_REPO.git\"\n",
                "# !git clone {REPO_URL} /kaggle/working/project\n",
                "# %cd /kaggle/working/project\n",
                "\n",
                "# Verify files\n",
                "!ls -la"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Install Ollama"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "# Install Ollama\n",
                "curl -fsSL https://ollama.com/install.sh | sh\n",
                "\n",
                "# Start Ollama server in background\n",
                "nohup ollama serve > /kaggle/working/ollama.log 2>&1 &\n",
                "\n",
                "# Wait for server to start (Kaggle needs more time)\n",
                "sleep 10\n",
                "\n",
                "# Verify Ollama is running\n",
                "curl http://localhost:11434/api/tags || echo \"Waiting for Ollama...\"\n",
                "sleep 5\n",
                "curl http://localhost:11434/api/tags"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Install Python Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q pyyaml jinja2 pandas pyarrow\n",
                "print(\"âœ“ Dependencies installed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Pull Models\n",
                "\n",
                "**Kaggle has 2x T4 GPUs (16GB total)** - You can use larger models than Colab!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Recommended for Kaggle (good balance of speed and quality)\n",
                "!ollama pull qwen3:4b          # 2.6 GB - fast and good quality\n",
                "!ollama pull deepseek-r1:8b    # 4.9 GB - best for reasoning\n",
                "\n",
                "# Optional: Add more for diversity\n",
                "# !ollama pull qwen3:8b        # 5.2 GB\n",
                "# !ollama pull phi4-mini       # 2.5 GB\n",
                "# !ollama pull gemma2:9b       # 5.4 GB\n",
                "\n",
                "# List available models\n",
                "!ollama list"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Test Run (Dry Run)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!python run_pipeline.py --max-seeds 2 --dry-run"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Small Test Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quick test to verify everything works\n",
                "!python run_pipeline.py \\\n",
                "    --model-strategy fixed \\\n",
                "    --model qwen3-4b \\\n",
                "    --ctx-mode fixed \\\n",
                "    --fixed-tokens 2048 \\\n",
                "    --max-seeds 5 \\\n",
                "    --samples-per-seed 2 \\\n",
                "    --output-dir /kaggle/working/output"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Full Generation\n",
                "\n",
                "Choose one configuration based on your goals:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration 1: MAXIMUM THROUGHPUT - Random models, mixed contexts\n",
                "!python run_pipeline.py \\\n",
                "    --model-strategy random \\\n",
                "    --ctx-mode profile \\\n",
                "    --max-seeds 100 \\\n",
                "    --samples-per-seed 3 \\\n",
                "    --output-format both \\\n",
                "    --output-dir /kaggle/working/output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration 2: QUALITY FOCUS - 8B model, long COT\n",
                "!python run_pipeline.py \\\n",
                "    --model-strategy fixed \\\n",
                "    --model deepseek-r1-8b \\\n",
                "    --ctx-mode long_cot \\\n",
                "    --max-seeds 50 \\\n",
                "    --samples-per-seed 3 \\\n",
                "    --output-format both \\\n",
                "    --output-dir /kaggle/working/output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration 3: SPEED - 4B model, short context\n",
                "!python run_pipeline.py \\\n",
                "    --model-strategy fixed \\\n",
                "    --model qwen3-4b \\\n",
                "    --ctx-mode fixed \\\n",
                "    --fixed-tokens 1024 \\\n",
                "    --max-seeds 200 \\\n",
                "    --samples-per-seed 2 \\\n",
                "    --output-format both \\\n",
                "    --output-dir /kaggle/working/output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration 4: SPECIFIC SKILLS - Focus on reasoning tasks\n",
                "!python run_pipeline.py \\\n",
                "    --skills RSN-ARITH RSN-LOGIC RSN-CAUSAL \\\n",
                "    --model-strategy fixed \\\n",
                "    --model deepseek-r1-8b \\\n",
                "    --ctx-mode profile \\\n",
                "    --max-seeds 30 \\\n",
                "    --samples-per-seed 3 \\\n",
                "    --output-format both \\\n",
                "    --output-dir /kaggle/working/output"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Check Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import os\n",
                "\n",
                "# List output files\n",
                "!ls -lh /kaggle/working/output/\n",
                "\n",
                "# Load and preview data\n",
                "output_dir = '/kaggle/working/output'\n",
                "if os.path.exists(output_dir):\n",
                "    for file in os.listdir(output_dir):\n",
                "        if file.endswith('.parquet'):\n",
                "            filepath = os.path.join(output_dir, file)\n",
                "            df = pd.read_parquet(filepath)\n",
                "            print(f\"\\n{'='*70}\")\n",
                "            print(f\"File: {file}\")\n",
                "            print(f\"Rows: {len(df):,}\")\n",
                "            print(f\"Columns: {list(df.columns)}\")\n",
                "            print(f\"\\nFirst 2 samples:\")\n",
                "            print(df.head(2))\n",
                "            print(f\"\\nData types:\")\n",
                "            print(df.dtypes)\n",
                "            print(f\"{'='*70}\")\n",
                "else:\n",
                "    print(\"No output directory found yet.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Download Results\n",
                "\n",
                "**Kaggle automatically saves files in `/kaggle/working/`**\n",
                "\n",
                "You can download them from the **Output** tab on the right â†’"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a summary of generated data\n",
                "import pandas as pd\n",
                "import json\n",
                "\n",
                "summary = {\n",
                "    \"total_samples\": 0,\n",
                "    \"files\": []\n",
                "}\n",
                "\n",
                "output_dir = '/kaggle/working/output'\n",
                "if os.path.exists(output_dir):\n",
                "    for file in os.listdir(output_dir):\n",
                "        if file.endswith('.parquet'):\n",
                "            filepath = os.path.join(output_dir, file)\n",
                "            df = pd.read_parquet(filepath)\n",
                "            summary[\"total_samples\"] += len(df)\n",
                "            summary[\"files\"].append({\n",
                "                \"name\": file,\n",
                "                \"rows\": len(df),\n",
                "                \"size_mb\": os.path.getsize(filepath) / (1024*1024)\n",
                "            })\n",
                "\n",
                "# Save summary\n",
                "with open('/kaggle/working/generation_summary.json', 'w') as f:\n",
                "    json.dump(summary, f, indent=2)\n",
                "\n",
                "print(\"\\nðŸ“Š Generation Summary:\")\n",
                "print(json.dumps(summary, indent=2))\n",
                "print(\"\\nâœ“ Summary saved to /kaggle/working/generation_summary.json\")\n",
                "print(\"âœ“ Download all files from the Output tab â†’\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. (Optional) Resume Generation\n",
                "\n",
                "If you need to continue generating more data:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!python run_pipeline.py \\\n",
                "    --resume \\\n",
                "    --max-seeds 50 \\\n",
                "    --output-dir /kaggle/working/output"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. (Optional) Push to HuggingFace Hub"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# First, install huggingface_hub\n",
                "!pip install -q huggingface_hub datasets\n",
                "\n",
                "# Set your HuggingFace token (get from https://huggingface.co/settings/tokens)\n",
                "HF_TOKEN = \"your_token_here\"\n",
                "REPO_NAME = \"your-username/dataset-name\"\n",
                "\n",
                "# Push to hub\n",
                "!python run_pipeline.py \\\n",
                "    --push-to-hub {REPO_NAME} \\\n",
                "    --hf-token {HF_TOKEN} \\\n",
                "    --max-seeds 100"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Monitor Progress"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check Ollama logs\n",
                "!tail -n 50 /kaggle/working/ollama.log"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU usage\n",
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check disk usage\n",
                "!df -h /kaggle/working"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Troubleshooting\n",
                "\n",
                "### Ollama Not Working"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if Ollama is running\n",
                "!ps aux | grep ollama\n",
                "\n",
                "# Check logs\n",
                "!cat /kaggle/working/ollama.log\n",
                "\n",
                "# Restart Ollama\n",
                "!pkill ollama\n",
                "!nohup ollama serve > /kaggle/working/ollama.log 2>&1 &\n",
                "!sleep 10\n",
                "!curl http://localhost:11434/api/tags"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Internet Not Working\n",
                "\n",
                "1. Click Settings (gear icon on right)\n",
                "2. Ensure **Internet** is **ON**\n",
                "3. Click **Save**\n",
                "4. Restart the notebook"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Out of Memory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use smaller models or reduce context\n",
                "!python run_pipeline.py \\\n",
                "    --model qwen3-4b \\\n",
                "    --ctx-mode fixed \\\n",
                "    --fixed-tokens 1024 \\\n",
                "    --max-seeds 10 \\\n",
                "    --output-dir /kaggle/working/output"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Performance Tips\n",
                "\n",
                "1. **Use 4B-8B models** for best speed/quality balance on Kaggle\n",
                "2. **Batch in chunks** of 50-100 seeds to avoid timeouts\n",
                "3. **Enable persistence** in settings to auto-save outputs\n",
                "4. **Monitor GPU quota** - Kaggle gives 30 GPU hours/week\n",
                "5. **Use `--resume`** to continue from checkpoints\n",
                "\n",
                "## Recommended Workflow\n",
                "\n",
                "1. **Test run** (5 seeds) - verify everything works\n",
                "2. **Small batch** (20-50 seeds) - check quality\n",
                "3. **Full generation** (100-200 seeds) - production run\n",
                "4. **Download** from Output tab\n",
                "5. **Repeat** if needed using `--resume`"
            ]
        }
    ],
    "metadata": {
        "kaggle": {
            "accelerator": "nvidiaTeslaT4",
            "dataSources": [],
            "dockerImageVersionId": 30646,
            "isGpuEnabled": true,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}