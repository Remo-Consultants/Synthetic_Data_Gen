{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COT Synthetic Data Generator - Google Colab\n",
    "\n",
    "This notebook runs the COT Synthetic Dataset Generator on Google Colab.\n",
    "\n",
    "**Setup Steps:**\n",
    "1. Change runtime to GPU (Runtime → Change runtime type → T4 GPU)\n",
    "2. Run all cells in order\n",
    "3. Download results from the last cell\n",
    "\n",
    "**Estimated Time:** 30-60 minutes for 20 seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your repository URL\n",
    "REPO_URL = \"https://github.com/YOUR_USERNAME/YOUR_REPO.git\"\n",
    "\n",
    "!git clone {REPO_URL}\n",
    "\n",
    "# Change to repository directory (update if different)\n",
    "%cd YOUR_REPO\n",
    "\n",
    "# Verify files\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Install Ollama\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Start Ollama server in background\n",
    "nohup ollama serve > ollama.log 2>&1 &\n",
    "\n",
    "# Wait for server to start\n",
    "sleep 5\n",
    "\n",
    "# Verify Ollama is running\n",
    "curl http://localhost:11434/api/tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Python Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pyyaml jinja2 pandas pyarrow\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pull Models\n",
    "\n",
    "**For Colab Free Tier (T4 GPU):** Use smaller models (1.7b, 2b, 4b)\n",
    "\n",
    "**For Colab Pro (A100):** You can use larger models (8b, 14b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommended for FREE tier\n",
    "!ollama pull qwen3:1.7b\n",
    "!ollama pull gemma2:2b\n",
    "\n",
    "# Uncomment for Colab Pro or if you want better quality\n",
    "# !ollama pull qwen3:4b\n",
    "# !ollama pull deepseek-r1:8b\n",
    "\n",
    "# List available models\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Run (Dry Run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_pipeline.py --max-seeds 2 --dry-run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Data\n",
    "\n",
    "Choose one of the configurations below based on your needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 1: FAST (Free Tier) - Small model, short context\n",
    "!python run_pipeline.py \\\n",
    "    --model-strategy fixed \\\n",
    "    --model qwen3-1.7b \\\n",
    "    --ctx-mode fixed \\\n",
    "    --fixed-tokens 1024 \\\n",
    "    --max-seeds 20 \\\n",
    "    --samples-per-seed 2 \\\n",
    "    --output-format both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 2: BALANCED (Free Tier) - Medium quality\n",
    "!python run_pipeline.py \\\n",
    "    --model-strategy random \\\n",
    "    --ctx-mode profile \\\n",
    "    --max-seeds 15 \\\n",
    "    --samples-per-seed 3 \\\n",
    "    --output-format both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 3: QUALITY (Colab Pro) - Best results\n",
    "# Uncomment if you have Colab Pro and pulled larger models\n",
    "# !python run_pipeline.py \\\n",
    "#     --model-strategy fixed \\\n",
    "#     --model deepseek-r1-8b \\\n",
    "#     --ctx-mode long_cot \\\n",
    "#     --max-seeds 30 \\\n",
    "#     --samples-per-seed 3 \\\n",
    "#     --output-format both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# List output files\n",
    "!ls -lh output/\n",
    "\n",
    "# Load and preview data\n",
    "output_dir = 'output'\n",
    "for file in os.listdir(output_dir):\n",
    "    if file.endswith('.parquet'):\n",
    "        df = pd.read_parquet(os.path.join(output_dir, file))\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"File: {file}\")\n",
    "        print(f\"Rows: {len(df)}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        print(f\"\\nSample:\")\n",
    "        print(df.head(2))\n",
    "        print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Create zip file\n",
    "shutil.make_archive('synthetic_data_output', 'zip', 'output')\n",
    "\n",
    "# Download\n",
    "files.download('synthetic_data_output.zip')\n",
    "\n",
    "print(\"✓ Download started! Check your browser's download folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. (Optional) Resume Generation\n",
    "\n",
    "If your session disconnects, re-run cells 1-4, then use this cell to resume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_pipeline.py \\\n",
    "    --resume \\\n",
    "    --max-seeds 20 \\\n",
    "    --output-format both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. (Optional) Monitor Ollama Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View last 50 lines of Ollama logs\n",
    "!tail -n 50 ollama.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Ollama Not Working\n",
    "```python\n",
    "# Check if Ollama is running\n",
    "!ps aux | grep ollama\n",
    "\n",
    "# Restart Ollama\n",
    "!pkill ollama\n",
    "!nohup ollama serve > ollama.log 2>&1 &\n",
    "!sleep 5\n",
    "```\n",
    "\n",
    "### Out of Memory\n",
    "- Use smaller models (qwen3:1.7b, gemma2:2b)\n",
    "- Reduce `--fixed-tokens` to 512 or 1024\n",
    "- Reduce `--max-seeds`\n",
    "\n",
    "### Session Timeout\n",
    "- Download results periodically\n",
    "- Use `--resume` flag to continue\n",
    "- Consider Colab Pro for longer sessions"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
